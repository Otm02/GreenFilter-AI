{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Otm02/GreenFilter-AI/blob/main/GreenFilter_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading"
      ],
      "metadata": {
        "id": "qloLxJK_SnV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dropout, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pickle\n",
        "from langdetect import detect, LangDetectException\n",
        "import openai\n"
      ],
      "metadata": {
        "id": "BdZTZhdaTNGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/My Drive/Colab Notebooks/AI EarthHack 2024/\""
      ],
      "metadata": {
        "id": "k3AaDCy_w-H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "qPUGc0PATA8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the provided CSV file (ours was edited using GPT 4 to add an isRelevant column based on its relevance to sustainability)\n",
        "df = pd.read_csv('Updated_EarthHack_Dataset.csv', encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "9ZaaMpBeqezi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = input(\"Enter your API key for the OpenAI GPT-3.5 API: \")\n",
        "\n",
        "# Initialize the OpenAI API client\n",
        "client = OpenAI(api_key=key)\n",
        "\n",
        "#Function to determine if the problem and solution are relevant to environmental sustainability. It returns 1 if relevant, otherwise 0.\n",
        "def is_relevant_to_environmental_sustainability(problem, solution):\n",
        "    prompt = f\"Problem: {problem}\\nSolution: {solution}\\nIs this problem and solution relevant to environmental sustainability? Answer 1 for Yes and 0 for No.\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "              {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        answer = response.choices[0].message\n",
        "        return 1 if answer == \"1\" else 0\n",
        "    except Exception as e:\n",
        "        print(f\"Error in processing: {e}\")\n",
        "        return 0\n",
        "\n",
        "# Add the column 'isRelevant' to the DataFrame\n",
        "df['isRelevant'] = df.apply(lambda row: is_relevant_to_environmental_sustainability(row['problem'], row['solution']), axis=1)"
      ],
      "metadata": {
        "id": "OOfytdSoqf1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRLQ6rFFvLHb"
      },
      "outputs": [],
      "source": [
        "# Function to remove odd characters and keep only ASCII\n",
        "def remove_odd_characters(text):\n",
        "    printable = set(string.printable)\n",
        "    return ''.join(filter(lambda x: x in printable, text))\n",
        "\n",
        "# Function to detect the language of the text using langdetect\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except LangDetectException:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Function to count the number of words\n",
        "def word_count(text):\n",
        "    return len(text.split())\n",
        "\n",
        "# Remove odd characters from problems and solutions\n",
        "df['problem'] = df['problem'].apply(lambda x: remove_odd_characters(str(x)))\n",
        "df['solution'] = df['solution'].apply(lambda x: remove_odd_characters(str(x)))\n",
        "\n",
        "# Convert text to lowercase for standardization\n",
        "df['problem'] = df['problem'].str.lower()\n",
        "df['solution'] = df['solution'].str.lower()\n",
        "\n",
        "# Filter out non-English entries and entries that are too short or too long\n",
        "df = df[(df['problem'].apply(detect_language) == 'en') &\n",
        "        (df['problem'].apply(word_count).between(5, 200)) &\n",
        "        (df['solution'].apply(word_count).between(5, 200))]\n",
        "\n",
        "# Drop rows with missing values in problem or solution\n",
        "df.dropna(subset=['problem', 'solution'], inplace=True)\n",
        "\n",
        "# Save the cleaned dataset\n",
        "df.to_csv('Cleaned_EarthHack_Dataset.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling"
      ],
      "metadata": {
        "id": "dEuFeJ0cUghs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfTrain = pd.read_csv('Cleaned_EarthHack_Dataset.csv')"
      ],
      "metadata": {
        "id": "8CtAkhoHVkTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifies the entries based on if they are relevant or not.\n",
        "class_counts = dfTrain['isRelevant'].value_counts()\n",
        "print(class_counts)"
      ],
      "metadata": {
        "id": "Br5lU2mUKrQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5040a7e0-6f6a-4372-daa4-2815360fb763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    737\n",
            "0    129\n",
            "Name: isRelevant, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "min_class_count = class_counts.min()\n",
        "\n",
        "# This is to clearly separate the two classes. This will be used for preprocessing.\n",
        "df_class_0 = dfTrain[dfTrain['isRelevant'] == 0]\n",
        "df_class_1 = dfTrain[dfTrain['isRelevant'] == 1]\n",
        "\n",
        "# Sample from the majority class\n",
        "df_class_0_under = df_class_0.sample(min_class_count)\n",
        "df_class_1_under = df_class_1.sample(min_class_count)\n",
        "\n",
        "#Now that we separated and balanced the entries (to avoid training it only for positive ideas, which can lead to it not detecting patterns)\n",
        "df_train_balanced = pd.concat([df_class_0_under, df_class_1_under], axis=0)\n",
        "\n",
        "# Shuffle the dataset\n",
        "df_train_balanced = df_train_balanced.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "dfTrain = df_train_balanced\n"
      ],
      "metadata": {
        "id": "wIwH-aGwKpTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#We're using tokenizer to generate numerical entries based on how common certain keywords are.\n",
        "tokenizer = Tokenizer()\n",
        "dfTrain['problem'] = dfTrain['problem'].astype('str')\n",
        "dfTrain['solution'] = dfTrain['solution'].astype('str')\n",
        "tokenizer.fit_on_texts(dfTrain['problem'] + ' ' + dfTrain['solution'])\n",
        "\n",
        "\n",
        "#The model requires a sequence.\n",
        "sequences = tokenizer.texts_to_sequences(dfTrain['problem'] + ' ' + dfTrain['solution'])\n",
        "\n",
        "#adjust this as needed, depending on the training data set.\n",
        "max_sequence_length = 206\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_length)\n"
      ],
      "metadata": {
        "id": "9O0E5iFhWDGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#Simply categorizes the entries on whether or not they are relevant or not, in a way that is good for tensorflow functions.\n",
        "y = to_categorical(dfTrain['isRelevant'])"
      ],
      "metadata": {
        "id": "P996EEJGYSw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#Splits it for training and testing. Validation is done later.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "QyRGC900bmaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape) #This is just to know the dimension. \"max_sequence_length\" should be equal to the number that is displayed down here."
      ],
      "metadata": {
        "id": "Q609C9MDtI-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bce67cc-c878-47ad-901f-a709c7c896da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(206, 206)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepping up the variables to be used in the parameters.\n",
        "sequence_length = X_train.shape[0]\n",
        "input_dim =  1\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "embedding_dim = 100\n",
        "\n",
        "print(\"sequence_length: {}\\ninput_dim: {}\".format(sequence_length, input_dim))\n",
        "#Adding the appropriate layers. LSTM is the recurrent part of our NN that we're gonna use to avoid the vanishing gradient problem.\n",
        "#Dropout introduces randomness to avoid overfitting (especially important with out small set of data...)\n",
        "#Dense is simply just several sub layers of neurons that all link to each other with their respective weights, all using the sigmoid activation, as we're addressing something binary.\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length = max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(64,return_sequences = True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units = 2, activation = 'sigmoid'))\n",
        "\n"
      ],
      "metadata": {
        "id": "8sbJzptCcPHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb5f3ca-7ecc-4b07-a519-7ace01c7e519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequence_length: 206\n",
            "input_dim: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sets the relevant parameters. This is generally what works for us.\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.learning_rate = 0.00001\n",
        "model.beta_1 = 0.9\n",
        "model.beta_2 = 0.999\n",
        "model.amsgrad = False"
      ],
      "metadata": {
        "id": "Nsa-lrNyhJrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#This is because it expects a (N, N, 1) format.\n",
        "X_train = np.expand_dims(X_train, axis=-1)"
      ],
      "metadata": {
        "id": "aTvL-nJUqt5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary()) #To ensure that the layers are formed, also to know what output shapes it wants for debugging."
      ],
      "metadata": {
        "id": "_AfMsJevrsNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20edeb16-6cc5-4a2d-dd3b-7c9a05f49e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 206, 100)          452000    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 206, 128)          84480     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 206, 128)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 64)                41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 577826 (2.20 MB)\n",
            "Trainable params: 577826 (2.20 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1) #The actual training for 20 epochs."
      ],
      "metadata": {
        "id": "WT66NoGHdi_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878fa307-7469-4595-f98a-c55c0474c642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(206, 206, 1)\n",
            "Epoch 1/20\n",
            "6/6 [==============================] - 13s 474ms/step - loss: 0.6899 - accuracy: 0.5514 - val_loss: 0.6835 - val_accuracy: 0.5238\n",
            "Epoch 2/20\n",
            "6/6 [==============================] - 1s 211ms/step - loss: 0.6717 - accuracy: 0.6378 - val_loss: 0.6714 - val_accuracy: 0.5714\n",
            "Epoch 3/20\n",
            "6/6 [==============================] - 1s 173ms/step - loss: 0.6233 - accuracy: 0.6919 - val_loss: 0.5884 - val_accuracy: 0.8095\n",
            "Epoch 4/20\n",
            "6/6 [==============================] - 1s 206ms/step - loss: 0.4475 - accuracy: 0.8432 - val_loss: 0.3257 - val_accuracy: 0.8571\n",
            "Epoch 5/20\n",
            "6/6 [==============================] - 1s 208ms/step - loss: 0.2683 - accuracy: 0.9135 - val_loss: 0.4191 - val_accuracy: 0.8571\n",
            "Epoch 6/20\n",
            "6/6 [==============================] - 1s 209ms/step - loss: 0.1429 - accuracy: 0.9838 - val_loss: 0.2814 - val_accuracy: 0.8571\n",
            "Epoch 7/20\n",
            "6/6 [==============================] - 2s 254ms/step - loss: 0.0661 - accuracy: 0.9946 - val_loss: 0.3588 - val_accuracy: 0.8571\n",
            "Epoch 8/20\n",
            "6/6 [==============================] - 2s 261ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.2921 - val_accuracy: 0.9048\n",
            "Epoch 9/20\n",
            "6/6 [==============================] - 2s 267ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.2624 - val_accuracy: 0.9048\n",
            "Epoch 10/20\n",
            "6/6 [==============================] - 1s 146ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.4567 - val_accuracy: 0.8571\n",
            "Epoch 11/20\n",
            "6/6 [==============================] - 1s 175ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.5749 - val_accuracy: 0.8571\n",
            "Epoch 12/20\n",
            "6/6 [==============================] - 1s 203ms/step - loss: 0.0269 - accuracy: 0.9946 - val_loss: 0.6579 - val_accuracy: 0.8571\n",
            "Epoch 13/20\n",
            "6/6 [==============================] - 1s 142ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 1.5663 - val_accuracy: 0.6667\n",
            "Epoch 14/20\n",
            "6/6 [==============================] - 1s 207ms/step - loss: 0.0402 - accuracy: 0.9892 - val_loss: 0.7917 - val_accuracy: 0.6667\n",
            "Epoch 15/20\n",
            "6/6 [==============================] - 1s 177ms/step - loss: 0.0226 - accuracy: 0.9946 - val_loss: 0.7041 - val_accuracy: 0.7143\n",
            "Epoch 16/20\n",
            "6/6 [==============================] - 1s 175ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.6845 - val_accuracy: 0.7619\n",
            "Epoch 17/20\n",
            "6/6 [==============================] - 1s 175ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.6950 - val_accuracy: 0.7619\n",
            "Epoch 18/20\n",
            "6/6 [==============================] - 1s 205ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.6826 - val_accuracy: 0.8095\n",
            "Epoch 19/20\n",
            "6/6 [==============================] - 2s 301ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.6641 - val_accuracy: 0.7619\n",
            "Epoch 20/20\n",
            "6/6 [==============================] - 1s 230ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.6452 - val_accuracy: 0.8095\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d09100e90c0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensuring that our testing data is formatted properly for evaluation.\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], sequence_length, input_dim))\n",
        "evaluation = model.evaluate(X_test_reshaped, y_test)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Test Accuracy: {evaluation[1]*100:.2f}%\")"
      ],
      "metadata": {
        "id": "oD2jOgZOhPwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32dc1f18-ae59-4227-80bb-5d6c9e017e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 27ms/step - loss: 0.6153 - accuracy: 0.8654\n",
            "Test Accuracy: 86.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is just so that we can visually see a heatmap for the subsequent block for the training data.\n",
        "predicted_classes1 = np.argmax(model.predict(X_test), axis = -1)\n",
        "y_true = np.argmax(y_test, axis = -1)"
      ],
      "metadata": {
        "id": "CPAqPyyV7eV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c445e9-9e31-45b2-f3fb-2f17aa3ec244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 2s 16ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "cm = confusion_matrix(y_true, predicted_classes1)\n",
        "plt.figure(figsize = (10,10))\n",
        "sns.heatmap(cm, annot = True, cbar = False)"
      ],
      "metadata": {
        "id": "N6YKxT-R7b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "outputId": "2991b40d-5ff3-48cb-a61a-509d4589350f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, predicted_classes1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlcWrLLhNfOi",
        "outputId": "9e9212ff-22d4-4f55-a8fd-ecb008f1d26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.85      0.86        26\n",
            "           1       0.85      0.88      0.87        26\n",
            "\n",
            "    accuracy                           0.87        52\n",
            "   macro avg       0.87      0.87      0.87        52\n",
            "weighted avg       0.87      0.87      0.87        52\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_and_predict(model, tokenizer, problem_text, solution_text, max_sequence_length):\n",
        "\n",
        "    input_text = np.array([problem_text + ' ' + solution_text])\n",
        "\n",
        "    input_text = input_text.astype('str')  # Ensure text is in string format\n",
        "    sequences = tokenizer.texts_to_sequences(input_text)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "\n",
        "    predictions = model.predict(padded_sequences)\n",
        "\n",
        "    predicted_class = np.argmax(predictions)\n",
        "\n",
        "    if predicted_class == 1:\n",
        "      print(\"The proposed problem and solution are relevant.\")\n",
        "    else:\n",
        "      print(\"The proposed problem and solution are not relevant.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0DFcW_tfuQ0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem = input(\"Propose a problem:\")\n",
        "solution = input(\"Propose a solution:\")\n",
        "\n",
        "preprocess_and_predict(model, tokenizer, problem, solution, 206)"
      ],
      "metadata": {
        "id": "fEU0tFQCzvqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85fe69a-b2d9-4074-ed8e-3a691d516602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Propose a problem:Largely considered the most urgent and impactful contemporary environmental crises by scientists and other experts, climate change is the most high-profile challenge facing the world today.\n",
            "Propose a solution:The single-most important thing that we can do to combat climate change is to drastically reduce our consumption of fossil fuels. The burning of coal, oil, and natural gas in our buildings, industrial processes, and transportation is responsible for the vast majority of emissions that are warming the planet—more than 75 percent. In addition to altering the climate, dirty energy also comes with unacceptable ecological and human health impacts. We must replace coal, oil, and gas with renewable and efficient energy sources.\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "The proposed problem and solution are relevant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "with open('tokenizerGreenFilter.pickle', 'wb') as filter:\n",
        "    pickle.dump(tokenizer, filter, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "gK0KGdqJzzOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"GreenFilterModel.json\", \"w\") as json_file:\n",
        "  json_file.write(model_json)"
      ],
      "metadata": {
        "id": "H32hiDXSJleI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WebUI"
      ],
      "metadata": {
        "id": "TdF1EDuAiYY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is pretty much identical to the one defined in Modelling, just so that for anyone that wants to run this and doesn't want to go through the hassle in the Modelling section in order to do so.\n",
        "def WebUIPredict(model, tokenizer, problem_text, solution_text):\n",
        "\n",
        "    input_text = np.array([problem_text + ' ' + solution_text])\n",
        "\n",
        "    input_text = input_text.astype('str')  # Ensure text is in string format\n",
        "    sequences = tokenizer.texts_to_sequences(input_text)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=206)\n",
        "\n",
        "\n",
        "    predictions = model.predict(padded_sequences)\n",
        "\n",
        "    predicted_class = np.argmax(predictions)\n",
        "    print(sequences)\n",
        "\n",
        "    if predicted_class == 1:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lUt4G-tjijfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk = Tokenizer()\n",
        "with open('tokenizerGreenFilter.pickle', 'rb') as handle:\n",
        "    tk = pickle.load(handle)"
      ],
      "metadata": {
        "id": "7DrZxeAsjxGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('GreenFilterModel.json', 'r') as json_file:\n",
        "  savedModel = json_file.read()"
      ],
      "metadata": {
        "id": "VtiyBOREkPHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelWB = tf.keras.models.model_from_json(savedModel)"
      ],
      "metadata": {
        "id": "7f6iaXQ5mwkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problemW = input(\"Propose a problem:\")\n",
        "solutionW = input(\"Propose a solution:\")\n",
        "\n",
        "WebUIPredict(modelWB, tk, problemW, solutionW)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6_L7hItk9O0",
        "outputId": "9292cb0a-3f6e-49d6-a170-efccc9379318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Propose a problem:Pollution is a well known issue, caused by the overabundance of greenhouse gas emitted by factories and vehicles.\n",
            "Propose a solution:A solution to this would be to go to the factories and force them to not dispose as many products through combustion. Furthermore, we can replace petroleum-based vehicles with electric vehicles.\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[[92, 10, 5, 387, 1418, 185, 891, 17, 1, 4, 276, 199, 17, 2, 331, 5, 20, 3, 8, 41, 12, 3, 556, 3, 1, 2, 2898, 63, 3, 27, 386, 15, 134, 25, 90, 270, 61, 11, 674, 2313, 100, 331, 13, 474, 331]]\n",
            "The proposed problem and solution are not relevant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir templates\n",
        "%cd templates\n",
        "!git clone -b ui https://github.com/Otm02/GreenFilter-AI ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KAwopOUuQaX",
        "outputId": "2fef23e3-7aa6-4708-9c8f-61f6bd63b3ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/templates\n",
            "Cloning into '.'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 39 (delta 14), reused 24 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (39/39), 1.62 MiB | 7.05 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "pkBC4c1buwNc",
        "outputId": "04f5712f-0399-456a-f7b4-c2ef25904efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5bdc5u5jsdn-496ff2e9c6d22116-5000-colab.googleusercontent.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Flask app\n",
        "from flask import Flask, render_template, jsonify, request\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "app = Flask(__name__, template_folder='.')\n",
        "\n",
        "# Define a route to render the HTML page\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('bot.html')\n",
        "\n",
        "# Run the Flask app\n",
        "@app.route('/solve', methods=['POST'])\n",
        "def get_response():\n",
        "    problem = request.form.get('problem')\n",
        "    print(problem)\n",
        "    solution = request.form.get('solution')\n",
        "    print(solution)\n",
        "\n",
        "    # Check if both parameters are provided\n",
        "    if problem is None or solution is None:\n",
        "        return jsonify({'error': 'Both \"problem\" and \"solution\" parameters are required'}), 400\n",
        "\n",
        "    # Process the parameters and generate a response\n",
        "    answer = \"test\" #WebUIPredict(modelWB, tk, problem, solution)\n",
        "\n",
        "    # Return the response as JSON\n",
        "    return jsonify({'answer': answer})\n",
        "\n",
        "    return jsonify(data)\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "metadata": {
        "id": "KmYTYZgblK7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a8de44-f32b-4632-f494-6a87be6ba1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:45:18] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:45:18] \"GET /static/main.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:45:18] \"GET /static/main.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:45:19] \"GET /static/img/bot.png HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:45:19] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:45:21] \"\u001b[31m\u001b[1mPOST /solve HTTP/1.1\u001b[0m\" 400 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:45:21] \"GET /static/img/guy.jpg HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:52:40] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:52:40] \"GET /static/main.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:52:40] \"GET /static/main.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:52:41] \"GET /static/img/bot.png HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:52:41] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:52:43] \"\u001b[31m\u001b[1mPOST /solve HTTP/1.1\u001b[0m\" 400 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:52:43] \"GET /static/img/guy.jpg HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:54:32] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:54:32] \"GET /static/main.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:54:32] \"GET /static/main.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:54:33] \"GET /static/img/bot.png HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:54:33] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:54:37] \"GET /static/img/guy.jpg HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:54:37] \"\u001b[31m\u001b[1mPOST /solve HTTP/1.1\u001b[0m\" 400 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:55:19] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:55:19] \"GET /static/main.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:55:19] \"GET /static/main.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:55:20] \"GET /static/img/bot.png HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:55:20] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:57:51] \"GET /static/img/guy.jpg HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Jan/2024 01:57:51] \"\u001b[31m\u001b[1mPOST /solve HTTP/1.1\u001b[0m\" 400 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}